customext_srcs += \
	spikema1s.cc \
	spiketa1s.cc \
	spiketama.cc \
	$(rvv_gen_agnostic_srcs) \

riscv_insn_ext_v_alu_int = \
	vaadd_vv \
	vaaddu_vv \
	vaadd_vx \
	vaaddu_vx \
	vadc_vim \
	vadc_vvm \
	vadc_vxm \
	vadd_vi \
	vadd_vv \
	vadd_vx \
	vand_vi \
	vand_vv \
	vand_vx \
	vasub_vv \
	vasubu_vv \
	vasub_vx \
	vasubu_vx \
	vcompress_vm \
	vcpop_m \
	vdiv_vv \
	vdiv_vx \
	vdivu_vv \
	vdivu_vx \
	vid_v \
	viota_m \
	vmacc_vv \
	vmacc_vx \
	vmadc_vv \
	vmadc_vx \
	vmadc_vi \
	vmadc_vim \
	vmadc_vvm \
	vmadc_vxm \
	vmadd_vv \
	vmadd_vx \
	vmand_mm \
	vmandn_mm \
	vmax_vv \
	vmax_vx \
	vmaxu_vv \
	vmaxu_vx \
	vmerge_vim \
	vmerge_vvm \
	vmerge_vxm \
	vfirst_m \
	vmin_vv \
	vmin_vx \
	vminu_vv \
	vminu_vx \
	vmnand_mm \
	vmnor_mm \
	vmor_mm \
	vmorn_mm \
	vmsbc_vv \
	vmsbc_vx \
	vmsbc_vvm \
	vmsbc_vxm \
	vmsbf_m \
	vmseq_vi \
	vmseq_vv \
	vmseq_vx \
	vmsgt_vi \
	vmsgt_vx \
	vmsgtu_vi \
	vmsgtu_vx \
	vmsif_m \
	vmsle_vi \
	vmsle_vv \
	vmsle_vx \
	vmsleu_vi \
	vmsleu_vv \
	vmsleu_vx \
	vmslt_vv \
	vmslt_vx \
	vmsltu_vv \
	vmsltu_vx \
	vmsne_vi \
	vmsne_vv \
	vmsne_vx \
	vmsof_m \
	vmul_vv \
	vmul_vx \
	vmulh_vv \
	vmulh_vx \
	vmulhsu_vv \
	vmulhsu_vx \
	vmulhu_vv \
	vmulhu_vx \
	vmv_s_x \
	vmv_v_i \
	vmv_v_v \
	vmv_v_x \
	vmv_x_s \
	vmv1r_v \
	vmv2r_v \
	vmv4r_v \
	vmv8r_v \
	vmxnor_mm \
	vmxor_mm \
	vnclip_wi \
	vnclip_wv \
	vnclip_wx \
	vnclipu_wi \
	vnclipu_wv \
	vnclipu_wx \
	vnmsac_vv \
	vnmsac_vx \
	vnmsub_vv \
	vnmsub_vx \
	vnsra_wi \
	vnsra_wv \
	vnsra_wx \
	vnsrl_wi \
	vnsrl_wv \
	vnsrl_wx \
	vor_vi \
	vor_vv \
	vor_vx \
	vredand_vs \
	vredmax_vs \
	vredmaxu_vs \
	vredmin_vs \
	vredminu_vs \
	vredor_vs \
	vredsum_vs \
	vredxor_vs \
	vrem_vv \
	vrem_vx \
	vremu_vv \
	vremu_vx \
	vrgather_vi \
	vrgather_vv \
	vrgather_vx \
	vrgatherei16_vv \
	vrsub_vi \
	vrsub_vx \
	vsadd_vi \
	vsadd_vv \
	vsadd_vx \
	vsaddu_vi \
	vsaddu_vv \
	vsaddu_vx \
	vsbc_vvm \
	vsbc_vxm \
	vsext_vf2 \
	vsext_vf4 \
	vsext_vf8 \
	vslide1down_vx \
	vslide1up_vx \
	vslidedown_vi \
	vslidedown_vx \
	vslideup_vi \
	vslideup_vx \
	vsll_vi \
	vsll_vv \
	vsll_vx \
	vsmul_vv \
	vsmul_vx \
	vsra_vi \
	vsra_vv \
	vsra_vx \
	vsrl_vi \
	vsrl_vv \
	vsrl_vx \
	vssra_vi \
	vssra_vv \
	vssra_vx \
	vssrl_vi \
	vssrl_vv \
	vssrl_vx \
	vssub_vv \
	vssub_vx \
	vssubu_vv \
	vssubu_vx \
	vsub_vv \
	vsub_vx \
	vwadd_vv \
	vwadd_vx \
	vwadd_wv \
	vwadd_wx \
	vwaddu_vv \
	vwaddu_vx \
	vwaddu_wv \
	vwaddu_wx \
	vwmacc_vv \
	vwmacc_vx \
	vwmaccsu_vv \
	vwmaccsu_vx \
	vwmaccu_vv \
	vwmaccu_vx \
	vwmaccus_vx \
	vwmul_vv \
	vwmul_vx \
	vwmulsu_vv \
	vwmulsu_vx \
	vwmulu_vv \
	vwmulu_vx \
	vwredsum_vs \
	vwredsumu_vs \
	vwsub_vv \
	vwsub_vx \
	vwsub_wv \
	vwsub_wx \
	vwsubu_vv \
	vwsubu_vx \
	vwsubu_wv \
	vwsubu_wx \
	vxor_vi \
	vxor_vv \
	vxor_vx \
	vzext_vf2 \
	vzext_vf4 \
	vzext_vf8 \

riscv_insn_ext_v_alu_fp = \
	vfadd_vf \
	vfadd_vv \
	vfclass_v \
	vfcvt_f_x_v \
	vfcvt_f_xu_v \
	vfcvt_rtz_x_f_v \
	vfcvt_rtz_xu_f_v \
	vfcvt_x_f_v \
	vfcvt_xu_f_v \
	vfdiv_vf \
	vfdiv_vv \
	vfmacc_vf \
	vfmacc_vv \
	vfmadd_vf \
	vfmadd_vv \
	vfmax_vf \
	vfmax_vv \
	vfmerge_vfm \
	vfmin_vf \
	vfmin_vv \
	vfmsac_vf \
	vfmsac_vv \
	vfmsub_vf \
	vfmsub_vv \
	vfmul_vf \
	vfmul_vv \
	vfmv_f_s \
	vfmv_s_f \
	vfmv_v_f \
	vfncvt_f_f_w \
	vfncvt_f_x_w \
	vfncvt_f_xu_w \
	vfncvt_rod_f_f_w \
	vfncvt_rtz_x_f_w \
	vfncvt_rtz_xu_f_w \
	vfncvt_x_f_w \
	vfncvt_xu_f_w \
	vfnmacc_vf \
	vfnmacc_vv \
	vfnmadd_vf \
	vfnmadd_vv \
	vfnmsac_vf \
	vfnmsac_vv \
	vfnmsub_vf \
	vfnmsub_vv \
	vfrdiv_vf \
	vfredmax_vs \
	vfredmin_vs \
	vfredosum_vs \
	vfredusum_vs \
	vfrec7_v \
	vfrsub_vf \
	vfrsqrt7_v \
	vfsgnj_vf \
	vfsgnj_vv \
	vfsgnjn_vf \
	vfsgnjn_vv \
	vfsgnjx_vf \
	vfsgnjx_vv \
	vfsqrt_v \
	vfslide1down_vf \
	vfslide1up_vf \
	vfsub_vf \
	vfsub_vv \
	vfwadd_vf \
	vfwadd_vv \
	vfwadd_wf \
	vfwadd_wv \
	vfwcvt_f_f_v \
	vfwcvt_f_x_v \
	vfwcvt_f_xu_v \
	vfwcvt_rtz_x_f_v \
	vfwcvt_rtz_xu_f_v \
	vfwcvt_x_f_v \
	vfwcvt_xu_f_v \
	vfwmacc_vf \
	vfwmacc_vv \
	vfwmsac_vf \
	vfwmsac_vv \
	vfwmul_vf \
	vfwmul_vv \
	vfwnmacc_vf \
	vfwnmacc_vv \
	vfwnmsac_vf \
	vfwnmsac_vv \
	vfwredosum_vs \
	vfwredusum_vs \
	vfwsub_vf \
	vfwsub_vv \
	vfwsub_wf \
	vfwsub_wv \
	vmfeq_vf \
	vmfeq_vv \
	vmfge_vf \
	vmfgt_vf \
	vmfle_vf \
	vmfle_vv \
	vmflt_vf \
	vmflt_vv \
	vmfne_vf \
	vmfne_vv \

riscv_insn_ext_v_ldst = \
	vlm_v \
	vle8_v \
	vle16_v \
	vle32_v \
	vle64_v \
	vloxei8_v \
	vloxei16_v \
	vloxei32_v \
	vloxei64_v \
	vlse8_v \
	vlse16_v \
	vlse32_v \
	vlse64_v \
	vluxei8_v \
	vluxei16_v \
	vluxei32_v \
	vluxei64_v \
	vle8ff_v \
	vle16ff_v \
	vle32ff_v \
	vle64ff_v \
	vl1re8_v \
	vl2re8_v \
	vl4re8_v \
	vl8re8_v \
	vl1re16_v \
	vl2re16_v \
	vl4re16_v \
	vl8re16_v \
	vl1re32_v \
	vl2re32_v \
	vl4re32_v \
	vl8re32_v \
	vl1re64_v \
	vl2re64_v \
	vl4re64_v \
	vl8re64_v \
	vsm_v \
	vse8_v \
	vse16_v \
	vse32_v \
	vse64_v \
	vsse8_v \
	vsoxei8_v \
	vsoxei16_v \
	vsoxei32_v \
	vsoxei64_v \
	vsse16_v \
	vsse32_v \
	vsse64_v \
	vsuxei8_v \
	vsuxei16_v \
	vsuxei32_v \
	vsuxei64_v \
	vs1r_v \
	vs2r_v \
	vs4r_v \
	vs8r_v \

riscv_insn_ext_v_ctrl = \
	vsetivli \
	vsetvli \
	vsetvl \

riscv_insn_ext_zvqdotq = \
	vqdot_vv \
	vqdot_vx \
	vqdotu_vv \
	vqdotu_vx \
	vqdotsu_vv \
	vqdotsu_vx \
	vqdotus_vx \

riscv_insn_ext_v = \
	$(riscv_insn_ext_v_alu_fp) \
	$(riscv_insn_ext_v_alu_int) \
	$(riscv_insn_ext_v_ctrl) \
	$(riscv_insn_ext_v_ldst) \
	$(riscv_insn_ext_zvqdotq) \

rvv_gen_agnostic_srcs = $(addsuffix _agnostic.cc,$(riscv_insn_ext_v))

$(rvv_gen_agnostic_srcs): %_agnostic.cc: $(src_dir)/riscv/insns/%.h $(src_dir)/customext/spiketama_insns.cc
	sed 's/NAME/$(subst _agnostic.cc,,$@)/' $(src_dir)/customext/spiketama_insns.cc > $@

